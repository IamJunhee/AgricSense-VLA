{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b168d25-91be-47fe-9690-9b6bdb6303c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iamjunhee/gemma_test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62331003-dfa0-4c33-be4e-1fa8b1892ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"/workspace/human_dataset\"\n",
    "\n",
    "def load_data_from_json_files(directory):\n",
    "    master_list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            master_list.append(data)\n",
    "    return master_list\n",
    "\n",
    "dataset = load_data_from_json_files(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e04fd-c63a-4b65-8956-8c20951b3c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b4fbd8ceb94c81a61bdf570564753b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "lora_id = \"IamJunhee/Gemma3-Agricsense_lora\"\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = lora_id,\n",
    "    max_seq_length = 20000,\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc48871-211b-4687-97b0-e22dc3a2901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=lora_id,     # directory to save and repository id\n",
    "    num_train_epochs=10,                         # number of training epochs\n",
    "    per_device_train_batch_size=1,              # batch size per device during training\n",
    "    gradient_accumulation_steps=4,              # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,                # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
    "    logging_steps=5,                            # log every 5 steps\n",
    "    save_strategy=\"epoch\",                      # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                         # learning rate, based on QLoRA paper\n",
    "    bf16=True,                                  # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                          # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                          # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",               # use constant learning rate scheduler\n",
    "    push_to_hub=True,                           # push model to hub\n",
    "    report_to=None,                    # report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  # use reentrant checkpointing\n",
    "    dataset_text_field=\"\",                      # need a dummy field for collator\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True}, # important for collator\n",
    "    remove_unused_columns=False, # important for collator\n",
    "    max_seq_length = 20000,\n",
    "    dataset_num_proc = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2696f-b52a-4405-b468-364efc0a8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def load_and_process_image(b64: str) -> str:\n",
    "    image_data_bytes = BytesIO(base64.b64decode(b64))\n",
    "    image = Image.open(image_data_bytes)\n",
    "    channels = len(image.getbands())\n",
    "    \n",
    "    if channels == 1:\n",
    "        img = np.array(image)\n",
    "        height, width = img.shape\n",
    "        three_channel_array = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "        if img.dtype == np.uint8:\n",
    "            img = img.astype(np.uint16)\n",
    "            img = ((img / 255) * 65535).astype(np.uint16)\n",
    "        \n",
    "        three_channel_array[:, :, 0] = (img // 1024) * 2\n",
    "        three_channel_array[:, :, 1] = (img // 32) * 8\n",
    "        three_channel_array[:, :, 2] = (img % 32) * 8\n",
    "        image = Image.fromarray(three_channel_array, \"RGB\")\n",
    "    \n",
    "    return image\n",
    "\n",
    "def process_vision_info(messages: list[dict]) -> list[Image.Image]:\n",
    "    image_inputs = []\n",
    "    # Iterate through each conversation\n",
    "    for msg in messages:\n",
    "        # Get content (ensure it's a list)\n",
    "        content = msg.get(\"content\", [])\n",
    "        if not isinstance(content, list):\n",
    "            content = [content]\n",
    "\n",
    "        # Check each content element for images\n",
    "        for element in content:\n",
    "            if isinstance(element, dict) and (\n",
    "                \"image\" in element or element.get(\"type\") == \"image\"\n",
    "            ):\n",
    "                # Get the image and convert to RGB\n",
    "                if \"base64\" in element:\n",
    "                    image = element[\"base64\"]\n",
    "                else:\n",
    "                    image = element\n",
    "                image_inputs.append(image)\n",
    "    \n",
    "    return [load_and_process_image(input).convert(\"RGB\") for input in image_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c26293-fc3b-45cd-a7a3-b5c2f1f8e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        image_inputs = process_vision_info(example[\"messages\"])\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        texts.append(text.strip())\n",
    "        images.append(image_inputs)\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = tokenizer(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens and image tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Mask image tokens\n",
    "    image_token_id = [\n",
    "        tokenizer.tokenizer.convert_tokens_to_ids(\n",
    "            tokenizer.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "        )\n",
    "    ]\n",
    "    # Mask tokens for not being used in the loss computation\n",
    "    labels[labels == tokenizer.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_del = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    try:\n",
    "        batch = collate_fn([dataset[i]])\n",
    "        token_len = batch[\"input_ids\"].shape[1]\n",
    "\n",
    "        if token_len > 25000:\n",
    "            indices_to_del.append(i)\n",
    "            print(f\"{i} : {token_len} -> will be deleted\")\n",
    "            continue\n",
    "\n",
    "        print(f\"{i} : {token_len}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{i} : error ({e}) -> will be deleted\")\n",
    "        indices_to_del.append(i)\n",
    "\n",
    "for index in sorted(indices_to_del, reverse=True):\n",
    "    del dataset[index]\n",
    "\n",
    "print(f\"Deletion complete. New dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ece97-cd1a-4d22-9d69-db66c9d27d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-29 07:24:16,844] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junhee0110/.agricsense/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:550: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['language_model.lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.data_collator = collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f98d6-09b2-4fc4-b798-ec817fb73b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   7/2500 01:13 < 10:10:21, 0.07 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>12.110200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e737162-1fa8-4797-9100-e41fa27fc37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
